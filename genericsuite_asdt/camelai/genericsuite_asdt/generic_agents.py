from typing import Union
import os
import yaml

from camel.agents import ChatAgent
from camel.models import BaseModelBackend
from camel.memories import (
    AgentMemory,
    ChatHistoryBlock,
    LongtermAgentMemory,
    ScoreBasedContextCreator,
    VectorDBBlock,
)
from camel.types import ModelType
from camel.utils import OpenAITokenCounter
from camel.toolkits import (
    SearchToolkit,
    CodeExecutionToolkit,
    HumanToolkit,
)
from camel.toolkits.function_tool import FunctionTool
from camel.societies import RolePlaying

from genericsuite_asdt.utils.app_logger import (
    log_debug,
    log_error,
    log_warning)
from genericsuite_asdt.utils.utilities import (
    get_file_or_text,
    remove_undesired_chars
)

from genericsuite_asdt.llm import get_llm_model_object

DEBUG = True
DEBUG_RAISE_EXCEPTION = False

# CAMELAI_AGENT_TOKEN_LIMIT = 4096
CAMELAI_AGENT_TOKEN_LIMIT = ""

# CAMELAI_MEMORY_TOKEN_LIMIT = 1024
CAMELAI_MEMORY_TOKEN_LIMIT = ""


class GsAutoGeneratedCrew():
    def __init__(self, params: dict = None):
        self.params: dict = params or {}
        self.error_message: str = None
        self.show_log_error: bool = self.params.get("show_log_error", True)
        self.set_own_memory: bool = self.params.get("set_own_memory", False)
        self.use_tools: Union[bool, None] = self.get_use_tools()

        self.kickoff_type: str = self.params.get("kickoff_type", "society")
        # self.kickoff_type: str = self.params.get("kickoff_type", "agent")

        self.agent_type: str = self.params.get("agent_type", "chat")

        self.tools: list = self.params.get("tools", self.get_tools())
        self.memory: AgentMemory = self.params.get("memory", self.get_memory())
        self.agent_token_limit = self.get_agent_token_limit()
        self.sys_msg = self.get_sys_msg()

        self.agents = {}
        self.tasks = {}
        self.agent_config_file = \
            self.params.get("agent_config_file",
                            os.environ.get("CAMELAI_AGENTS_CONFIG_FILE"))
        self.task_config_file = \
            self.params.get("task_config_file",
                            os.environ.get("CAMELAI_TASK_CONFIG_FILE"))

        self.llm: BaseModelBackend = self.get_model()
        self.manager_agent_llm: BaseModelBackend = \
            get_llm_model_object("manager")
        self.coding_agent_llm: BaseModelBackend = \
            get_llm_model_object("coding")
        self.reasoning_agent_llm: BaseModelBackend = \
            get_llm_model_object("reasoning")
        self.planning_agent_llm: BaseModelBackend = \
            get_llm_model_object("planning")

        self.agent = self.get_configured_agent()

    def get_error_message(self) -> Union[str, None]:
        return self.error_message

    def report_error(self, error_obj: Exception = None) -> None:
        if self.show_log_error:
            log_error(f"GS-ERROR: {self.error_message}")
        if DEBUG_RAISE_EXCEPTION and error_obj:
            raise error_obj

    def get_use_tools(self):
        if self.params.get("use_tools"):
            return self.params["use_tools"]
        return os.environ.get("USE_TOOLS", "true") == "true"

    def get_model(self) -> BaseModelBackend:
        log_debug("Camel-AI Get Model")
        try:
            model = get_llm_model_object()
        except Exception as e:
            self.error_message = f"Getting model: {e}"
            self.report_error(e)
            model = None
        return model

    def get_agent_token_limit(self) -> int:
        log_debug("Camel-AI Get Token Limit")
        token_limit = os.environ.get(
            "CAMELAI_AGENT_TOKEN_LIMIT",
            CAMELAI_AGENT_TOKEN_LIMIT)
        return int(token_limit) if token_limit.isdigit() else None

    def get_sys_msg(self) -> Union[str, None]:
        log_debug("Camel-AI Get Sys Msg")
        file_or_text = get_file_or_text(
            os.environ.get(
                "CAMELAI_AGENT_SYS_MSG",
                "You are a helpful software engineer assistant."))
        if file_or_text['error']:
            self.error_message = \
                f'Project cannot be loaded from file' \
                f' "{file_or_text["file_path"]}":' \
                f' {file_or_text["error_message"]}'
            self.report_error()
            return None
        else:
            return file_or_text['content']

    def get_tools(self) -> list[FunctionTool]:
        """
        Initialize default tools
        """
        log_debug("Camel-AI Get Tools")
        tools = []
        if not self.use_tools:
            return tools

        # Initialize Search toolkit
        toolkit = SearchToolkit()
        tool_func = FunctionTool(toolkit.search_google)
        tools.append(tool_func)

        # Initialize Code Execution toolkit
        toolkit = CodeExecutionToolkit()
        tool_func = FunctionTool(toolkit.execute_code)
        tools.append(tool_func)

        toolkit = HumanToolkit()
        tool_func = FunctionTool(toolkit.ask_human_via_console)
        tools.append(tool_func)

        return self.tools_clean(tools)

    def tools_clean(self, tools):
        """
        Clean tools by removing OpenAI completions invalid parameters
        """
        for tool in tools:
            function_dict = tool.get_openai_function_schema()
            if 'strict' in function_dict:
                del function_dict['strict']
            tool.set_openai_function_schema(function_dict)
        return tools

    def get_memory(self) -> AgentMemory:
        """
        Initialize default memory
        """
        log_debug("Camel-AI Get Memory")
        token_limit = os.environ.get(
            "CAMELAI_MEMORY_TOKEN_LIMIT",
            CAMELAI_MEMORY_TOKEN_LIMIT)
        if self.set_own_memory:
            memory = LongtermAgentMemory(
                context_creator=ScoreBasedContextCreator(
                    token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),
                    token_limit=token_limit,
                ),
                chat_history_block=ChatHistoryBlock(),
                vector_db_block=VectorDBBlock(),
            )
        else:
            memory = None
        return memory

    def get_configured_agent(
        self,
    ) -> Union[ChatAgent, None]:
        """
        Get the configured agent.
        """
        _ = DEBUG and log_debug(
            "Camel-AI Get Configured Agent")

        if self.error_message:
            log_warning("Agent cannot be initialized due a previous error: " +
                        self.error_message)
            return

        self.agent = None
        if self.agent_type == "chat":
            params = {
                "system_message": self.sys_msg,
                "model": self.llm,
            }
            if self.set_own_memory:
                params["memory"] = self.memory
            if self.use_tools:
                params["tools"] = self.tools
            if self.agent_token_limit:
                params["token_limit"] = self.agent_token_limit
            agent = ChatAgent(**params)
        else:
            self.error_message = f"Agent type {self.agent_type} not supported"
            self.report_error()
        return agent

    def crew(self) -> Union[ChatAgent, None]:
        return self

    def get_agent_response(self, query: str) -> str:
        if self.error_message:
            log_warning("Agent cannot run due a previous error: " +
                        self.error_message)
            return
        try:
            response = self.agent.step(query)
        except Exception as e:
            self.error_message = f"Agent RUN: {str(e)}"
            self.report_error(e)
            return self.error_message
        _ = DEBUG and \
            log_debug(f"Camel-AI Assistant Response: {response}")
        if len(response.msgs) == 0:
            return "Termination reasons: " + \
                ", ".join(response.info.get(
                    "termination_reasons",
                    ["CGAGC-E010: unknown reason"])
                    ) + " - Num tokens: " + \
                str(response.info.get("num_tokens", -1))
        return response.msgs[0].content

    def get_task(self, task_prompt: str, with_task_specify: bool = True,
                 model: BaseModelBackend = None) -> dict:
        if not model:
            model = self.llm
        task_kwargs = {
            'task_prompt': task_prompt,
            'with_task_specify': with_task_specify,
            'task_specify_agent_kwargs': {'model': model},
        }
        return task_kwargs

    def get_user_role(self, user_role_name: str,
                      model: BaseModelBackend = None) -> dict:
        if not model:
            model = self.llm
        user_role_kwargs = {
            'user_role_name': user_role_name,
            'user_agent_kwargs': {'model': model},
        }
        return user_role_kwargs

    def get_assistant_role(self, assistant_role_name: str,
                           model: BaseModelBackend = None) -> dict:
        if not model:
            model = self.llm
        assistant_role_kwargs = {
            'assistant_role_name': assistant_role_name,
            'assistant_agent_kwargs': {'model': model},
        }
        return assistant_role_kwargs

    def is_terminated(self, response):
        """
        Give alerts when the session should be terminated.
        """
        if response.terminated:
            role = response.msg.role_type.name
            reason = response.info['termination_reasons']
            print(f'AI {role} terminated due to {reason}')

        return response.terminated

    def run_society(self, society, round_limit: int = 10):

        # Get the initial message from the ai assistant to the ai user
        input_msg = society.init_chat()

        # Starting the interactive session
        for _ in range(round_limit):

            # Get the both responses for this round
            assistant_response, user_response = society.step(input_msg)

            # Check the termination condition
            if self.is_terminated(assistant_response) or \
               self.is_terminated(user_response):
                break

            # Get the results
            print(f'[AI User] {user_response.msg.content}.\n')

            # Check if the task is end
            if 'CAMEL_TASK_DONE' in user_response.msg.content:
                break

            print(f'[AI Assistant] {assistant_response.msg.content}.\n')

            # Get the input message for the next round
            input_msg = assistant_response.msg

        return None

    def get_agents(self) -> list:
        """
        Loads the agents from the agent config file.

        Returns:
            A list of agents.
        """
        if not self.agent_config_file:
            self.error_message = "agent_config_file is not set"
            return []

        try:
            with open(self.agent_config_file, "r") as f:
                agents_config = yaml.safe_load(f)
        except Exception as e:
            self.error_message = f"Could not load agents: {e}"
            return []

        if not agents_config:
            self.error_message = "Could not load agents: config is empty"
            return []

        agents = []
        for agent_name, agent_config in agents_config.items():
            _ = DEBUG and log_debug(
                f">> Loading agent: {agent_name}"
                f"\n| agent_config: {agent_config}")
            if agent_name == "manager":
                self.agents[agent_name] = dict(agent_config)
                self.manager_agent = self.agents[agent_name]
                continue
            self.agents[agent_name] = dict(agent_config)
            agents.append(self.agents[agent_name])
        return agents

    def get_tasks(self) -> list:
        if not self.task_config_file:
            self.error_message = "task_config_file is not set"
            return []
        try:
            with open(self.task_config_file, "r") as f:
                tasks_config = yaml.safe_load(f)
        except Exception as e:
            self.error_message = f"Could not load tasks: {e}"
            return []
        tasks = []
        self.task_sequence = 0
        for task_name, task_config in tasks_config.items():
            _ = DEBUG and log_debug(
                f">> Loading task: {task_name}"
                f"\n| task_config: {task_config}")
            self.tasks[task_name] = dict(task_config)
            tasks.append(self.tasks[task_name])
        return tasks

    def get_society_response(self, inputs: dict) -> str:
        self.get_agents()
        if self.error_message:
            return
        self.get_tasks()
        if self.error_message:
            return

        task_prompt = next(iter(self.tasks.values()))['description']
        task_prompt = task_prompt.replace('{project}', inputs['project'])
        task_prompt = task_prompt.replace('{topic}', inputs['topic'])
        task_prompt = remove_undesired_chars(task_prompt,
                                             replacements="{}")

        assistant_role = self.agents.get("senior_software_engineer", {
            "backstory": "As a Senior Software Engineer, you are responsible"
            " for ensuring high-quality software by writing clean, efficient,"
            " and modular code. You work with various programming languages"
            " and frameworks."
        })['backstory']

        user_role = self.agents.get("final_user", {
            "backstory": "A software developer requiring the Society's"
            " assistance."
        })['backstory']

        task_kwargs = self.get_task(task_prompt)
        user_role_kwargs = self.get_user_role(user_role)
        assistant_role_kwargs = self.get_assistant_role(assistant_role)

        _ = DEBUG and log_debug(
            ">> Running Society:"
            f"\n | task_kwargs: {task_kwargs}"
            f"\n | user_role_kwargs: {user_role_kwargs}"
            f"\n | assistant_role_kwargs: {assistant_role_kwargs}"
        )

        society = RolePlaying(
            **task_kwargs,             # The task arguments
            **user_role_kwargs,        # The instruction sender's arguments
            **assistant_role_kwargs,   # The instruction receiver's arguments
        )

        return self.run_society(society)

    def kickoff(self, inputs: dict) -> None:
        if self.error_message:
            return
        if self.kickoff_type == "agent":
            response = self.get_agent_response(inputs["project"])
        elif self.kickoff_type == "society":
            response = self.get_society_response(inputs)
        else:
            self.error_message = \
                f"Kickoff type {self.kickoff_type} not supported"
        if self.error_message:
            self.report_error()
        else:
            print(response)

    def replay(self, task_id: str) -> None:
        if self.error_message:
            return
        pass

    def train(
        self,
        n_iterations: int,
        filename: str,
        inputs: dict
    ) -> None:
        if self.error_message:
            return
        pass

    def test(
        self,
        n_iterations: int,
        openai_model_name: str,
        inputs: dict
    ) -> None:
        if self.error_message:
            return
        pass


class GenericsuiteAsdtCrew(GsAutoGeneratedCrew):
    pass
